# makemore

The next part in Andrej Karpathy’s Youtube Series is an introduction to language modeling. That is, being given data of words, how can we create a model to predict words like it? For example, given a dataset of human names, how can we train a model to predict more unique names? Hence the name “makemore.”

# Part 1 - Intro to Language Modeling: Bigram Language Model
## Bigram Statistical Model
There are several ways to create this model, some more complex, modern and accurate, some less powerful and outdated, that don’t even use neural networks. We will start with an older way that isn’t very accurate, and work our way up. The first thing we’ll do is create a bigram language model. Bigram models use two characters side by side and use probabilities to predict the next character in the sequence to come out with a word. It ignores the words that came before its previous one. And as a result of that, it ends up not being very accurate, yet it uses many fascinating techniques with statistics and probability in interesting ways.

### Exploring the Dataset
So first we load up our dataset, ‘names.txt.’ It is a file of 32K names. Then we put it into a dictionary. We put each bigram of each word into a dictionary, and count the frequency of each bigram. The dictionary’s key is the bigram, stored as a tuple, and the value is the count. Now for this, we have to remember that the leading and ending characters of the word are unique, and we have to signify that these words are essentially preceded by the starting place and succeeded essentially by the ending place. For this, we add to the list the bigram of (start symbol, first letter) and (last letter, end symbol). That is how we store that data. We decide to use a ‘.’ For both starting and ending places. The position of the period will indicate if it’s starting or ending. So okay, we have a dictionary that tell us how many times a bigram was seen in our data.
But this isn’t exactly how we want to store our data. We actually want to use a 2D array of pytorch tensors, specifically a 27 x 27 matrix. We can use the row and column indices to indicate which two characters are the bigram, and the value to be its frequency. So for example, the index 0 represents a ‘.’, then 1 represents ‘a,’ all the way till the last index 26 representing ‘z.’ So the value at [1, 1] will represent the number of times we’ve seen the bigram ‘aa’ in our data. And the value at [2, 0] will represent ‘b.’, which is the number of times ‘b’ was the last letter of a word. The value at [0, 3] represents ‘.c’, and that’s the number of times a word started with ‘c.’ We go through each word in the dataset and update that matrix, which we call N. Then we graph it.

### Coding the Model Built with Probability
What we want to do is find the probability that, for any given character, what is the probability that any character follows it. We do this starting with the first character, ‘.{char}' (this represents the first character), given in the first row of the matrix. Once we get the first character, we want to go to that character’s row, get the probability distribution for what character is likely to follow that character. Remember that each character has a probability that it is the last one, represented by ‘{char}.’ We use the random function in pytorch to get unique, random values. We continue this until the character ends, and we have a sample name with bigrams.
To do this, we next need to normalize our counts matrix so that each row adds up to one. This is our probabilities. Then we write code to randomly pick a character based on the probability distribution using .multinomial() from pytorch until we reach the terminating character ‘.’ signifying the word is done. We code more and we’re done! I learned also about broadcasting, which is a term that describes what the computer does under the hood for matrix operations. Karpathy explains that it is very important to learn how to do matrix operations, specifically for ML frameworks, since we’ll be doing many many operations and functions for neural networks and data handling. Doing these functions is desirable because matrix operations are way faster.
When we code the thing up, we see that it works well, at least relatively. I got some interesting decent names like “junide,” “janasah”, and “kohin”, but also some weird or not so great names such as “ksahnaauranilevias” and “ssorionsush.” So this model isn’t great, as we expected.

### Evaluating Our Model
To evaluate our model, Andrej introduces something called Maximum Likelihood Estimation. So if we multiply the probabilities of each bigram in a word, we get the likelihood that that character is outputted, based on our probabilities in our model. Remember that these probabilities are kind of like weights in a neural net, in that they can change. The Maximum Likelihood Estimation (MLE) is a method to tweak the probabilities to increase the likelihood that our model predicts names from our data. The likelihood is like a loss function that tells us how good our model is, except that high is good and low is bad. One way to measure likelihood is by taking the product of all the probabilities of each bigram. Now that wouldn’t be practical for us since that would give us way too small numbers. So instead, we’ll use the log likelihood, which instead takes the log of each probability and sums them up. In this way, a small value (negative) means it’s doing bad, and a more positive value (closer to 0) means it’s good. Now, we don’t quite want that. We want a value that resembles a loss function, that is, a positive value that the larger it is, is bad, and the smaller it is, is good. Perfectly, we just use the negative log likelihood. And now, we get a value that represents when the value is high, it’s a high loss, and when it’s low, it’s doing well, just like a regular loss. But there’s still one more thing to do. People like to normalize this value and make it smaller, by dividing by the number of bigrams. And we’re done. The average negative log likelihood. This is the loss that we want to minimize when training our model. The next step is training our model to maximize the likelihood by changing the probabilities of the bigrams, aka minimizing the average negative log likelihood. Oh one more thing. Since there are zeros in our probability, it’s possible that we get a -infinity average negative log likelihood. This is undesirable. We’d like to get a very unlikely value, but not zero. To fix this, we simply add 1 to each count, which gives each probability a nonzero value. This is called model smoothing. We essentially synthetically change the data to give us desirable results. We can add a larger value to each count and that would smooth our model more if we wanted.

Wait… So why is this model so bad? Thinking back, we took counts of every bigram of all the words in our dataset, and just took their probabilities. It seems sensible. But thinking about how names are formed and what makes a “good” name, it doesn’t seem quite as simple as using probabilities and choosing higher probability bigrams to put as the next character in a name. I mean ok it makes a lot of sense intuitively, this approach, but as in the example of the generated name “ksahnaauranilevias,” it looks like the algorithm didn’t recognize when to stop the name and it just went bonkers with vowels in the middle. A big problem is that it only looks at the previous letter for example the ‘a’ in ‘aa’, and it doesn’t recognize that firstly there are already two vowels behind is and secondly that the name is already pretty long and getting weird (no disrespect if this name is common in a certain area in the world). If it can evaluate the entire word it has generated thus far, it would do a much better job. Anyways, next we will implement another bigram model, but this time with a neural network. Let’s see how we do it!

## Bigram Model with a Neural Network
There is another way to create a bigram model, and that is with deep learning. It’s useful to practice using neural nets since they are really powerful. To do this, we need to find out how to represent our data, define our loss function, how to make our model and how to train it.

### Representing the Data
Our data is still just bigrams. For neural nets, we need training data, that is x values, which is the inputs, and y values, which is the labels or ground truths. We will make the first letter in the bigram the x value, the input, and the second letter of the bigram the y value, the output label. We will essentially train the model by telling it that from our dataset, we want these certain bigrams to be predicted more, and bigrams that weren’t in our dataset much to be predicted less. We store the x values and y values in arrays, or more specifically, tensors, with the i-th index of each array corresponding to the i-th example. So we’ve defined the data as an integer, for example x = [5], y = [13] for the bigram ‘em’ in ‘emma.’ However, we don’t want this. Our data is actually categorical and not numerical. The input and output is one of 27 options, the 26 letters and the ‘.’ character. We don’t want our model, when trained, to develop any numerical relationship. For this reason, we actually represent our input and output data with something called One-Hot Encoding. The data will be an array of size 27 all with zeros except for the input character. It will be given 1. Same with the output. This is the common practice for categorical data. Also we make them floats because we want the numbers to be floats when passing through the neural net.

Now we move on to understanding the matrix operations used for the forward pass in our neural net. For example, if we have a 5 x 27 matrix of 5 training examples (each example has 27 inputs or features), and our first hidden layer has 27 neurons, each neuron will have 27 weights. So the first layer will be a 27 x 27 matrix, with the columns being all the weights of a neuron. If we think about doing a matrix multiplication on these two matrices, 5 x 27 @ 27 x 27 returns a 5 x 27 matrix, based on the rules of matrix multiplication. If we think about the first training example, which is the first row in the 5 x 27 matrix (the first row is the 27 features that are one-hot encoded), it does a dot product on all 27 columns of the 27 x 27 weights matrix, and that returns the first row of 27 values. Each value represents the dot product of the inputs and the weights for each neuron for just the first example. Then all other example are calculated, and we end up with the 5 x 27 matrix. So if we look at the value in [3, 10] of that matrix, that value represents the output of the 10th neuron for example number 3. And that value is just a scalar.
So since we initialize our model with random weights, we expect random outputs in the 5 x 27 matrix. We get positives and negatives. Remember that the output represents a probability distribution that tells us which character our model is predicting as the next character in the bigram. So we want the 27 outputs to be between 0 and 1 and to all sum to 1, just like how we did in the bigram statistical model. To squash our values, or normalize them, we do something called a Softmax. We exponentiate the values, which is do e^x. This makes them all nonnegative. And then we just squash them by dividing them all by the horizontal sum. Now they all sum to one and represent probabilities.

So this is the summary for what we have so far: First, we took the 5 example bigrams and put them into a x array tensor that stores the first character. The second character goes in the y label array tensor. We are looking at each bigram independent of the word it belongs to. Then we one-hot encode each example so each input has 27 parameters. We create the first layer of our neural net. It has 27 neurons and each neuron has 27 weights. We perform matrix multiplication on all the training example simultaneously with the weights of the first layer, and we end up with our output, which is 27 outputs. To get the outputs to be probability distributions, we take the softmax function, that is exponentiating the outputs and then normalizing the outputs by dividing it by the sum of the outputs so that they add up to 1.

### Optimizing/Training Our Neural Network
Next, we write some code to view the losses for each of the bigrams in a word. Then, let’s go about training our model. So as we’ve done for micrograd, the three steps we did to train our model was firstly performing a forward pass, secondly zeroing the gradients and performing a backward pass, aka backpropagation to get the gradients of each node, and thirdly nudging the weights. And repeat until we get a desirable loss.
So first we code the forward pass. We have done this before. We do the one-hot encoding, the matrix multiplication and the softmax function. The final step of the forward pass is actually deriving the loss output value. The loss is derived from taking the log of the model’s probability for the example. If the probability is close to 1, you get a loss close to 0. If it’s close to 0, it’s bad, and it approaches infinity, a high loss. That is the average negative log likelihood, and we vectorize all of it, meaning we calculate that in parallel for all training examples in one line of code. So that is the forward pass: the forward pass of the network and then the forward of the loss.
Next is the backward pass. We zero the gradients and then just call loss.backward()
Finally, the nudging of the weights. We choose a learning rate and here we update the weights in one fell swoop with the vectorized operation from pytorch.
We write all of our steps, boiled down to creating the dataset and then performing gradient descent in a loop, and when printing the loss for each iteration, we see it converging roughly to 2.5.

Then we run forward passes using the trained model and generate some names. This is called sampling from the model. They are very similar to what we got in the statistical bigram model, but unlike Karpathy, I actually got a few different results. I think I trained it more and spammed the gradient descent loop much more than he did. I ended up with 2.48 loss. But for the most part it’s the same. I also did more outputs. Maybe his would be slightly different too.

### Overall Summary
Andrej Karpathy draws many parallels to the statistical model and the second version with the neural net. Similar to how we chose the row of probabilities for the first character we were on in the statistical model, it turns out we are doing the same thing in the neural network version. Since we use one-hot encoding, when we do the dot products, all terms that are 0 are gotten rid of, and we only retain the value with the 1 for the 27 neurons, and we end up with those probabilities. In addition to how we applied smoothing in the statistical model to get rid of bigrams with 0% probability of occurring, we can also apply regularization in our model, and he explains it as getting the weights close to zero. Now, I don’t get how regularizing the loss function gets the weights to zero, but it made sense. Model smoothing is pretty much the same as regularization (I think). It’s really cool that he describes why the two models are so alike. Finally, he notes that the neural network version has much more potential to scale up and to take more inputs to handle not just bigrams, but essentially any length of characters, making neural networks much more powerful and much more scalable for this task.
